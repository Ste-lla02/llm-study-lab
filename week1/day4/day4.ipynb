{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Querying deepseek-r1:1.5b locally with Ollama Using the OpenAI Library ü¶ôüìñ\n",
    "\n",
    "This notebook shows how to use the openai Python library as an OpenAI-compatible client to query a language model running locally via Ollama.\n",
    "\n",
    "## Objective üéØ\n",
    "- Query a **local LLM** (via Ollama) using the same calling primitives typically adopted in the OpenAI ecosystem.\n",
    "- Avoid the use of **API keys**: inference is performed entirely on your local machine through a local endpoint.\n",
    "\n",
    "## Why this works üë©üèª‚Äçüíª\n",
    "Ollama can expose a local endpoint that is compatible with the OpenAI API specification. In practice:\n",
    "1. the OpenAI library sends a standard HTTP request,\n",
    "2. Ollama receives the request on localhost and generates the response using a locally installed model.\n",
    "This setup allows us to decouple the client library from the model provider, while preserving a unified interface."
   ],
   "id": "c819bb97d5016b51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prerequisites üìú\n",
    "1. Install Ollama and make sure it is available on your system.\n",
    "2. Start the Ollama service (if it is not already running):\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "3. Check the list of available local models:\n",
    "```bash\n",
    "ollama list\n",
    "```\n",
    "4. If needed, download a model:\n",
    "```bash\n",
    "!ollama pull deepseek-r1:1.5b\n",
    "```"
   ],
   "id": "2a900c80073f0de0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Download the model\n",
    "!ollama pull deepseek-r1:1.5b"
   ],
   "id": "1ebd08970e32c80a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"deepseek-r1:1.5b\", messages=[{\"role\":\"user\", \"content\":\"Hi deepseek, can you tell me a scary story?\"}])\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "7754a6a57eb79014"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

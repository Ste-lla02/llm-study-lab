{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise goal ‚Äì The OpenAI Python library ü§ñüì¶\n",
    "\n",
    "In the following example, we use the OpenAI to interact\n",
    "with different Large Language Models.\n",
    "\n",
    "In this notebook we will:\n",
    "- interact with **Gemini** via API (cloud-based inference),\n",
    "- interact with **Ollama** using local inference.\n",
    "\n",
    "Both approaches expose the same conceptual workflow:\n",
    "prompt ‚Üí inference ‚Üí completion,\n",
    "but differ in **where inference happens** and **how models are accessed**.\n"
   ],
   "id": "716cb393711d1fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests"
   ],
   "id": "2f4c049a12026d8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Google Gemini via API üîë\n",
    "\n",
    "To interact with Gemini models, inference is performed remotely on Google‚Äôs\n",
    "infrastructure. This requires an **API key**.\n",
    "\n",
    "#### Step 1 ‚Äì Create a Gemini API key\n",
    "\n",
    "Generate the API key here: https://aistudio.google.com/api-keys\n",
    "\n",
    "#### Step 2 ‚Äì Store the API key safely\n",
    "\n",
    "Add the key to the `.env` file: `GOOGLE_API_KEY=AIza...`\n",
    "\n",
    "---\n",
    "## üåç Conceptual note: Gemini inference\n",
    "\n",
    "When using Gemini:\n",
    "- the notebook runs locally,\n",
    "- the model runs remotely,\n",
    "- prompts and responses are exchanged via API calls.\n",
    "\n",
    "This is an example of **cloud-based inference**:\n",
    "no local GPUs are required, but internet access and credentials are mandatory.\n"
   ],
   "id": "1cf2db28974eadff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not google_api_key:\n",
    "    print(\"No API key was found - please be sure to add your key to the .env file, and save the file! Or you can skip the next 2 cells if you don't want to use Gemini\")\n",
    "elif not google_api_key.startswith(\"AIz\"):\n",
    "    print(\"An API key was found, but it doesn't start AIz\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ],
   "id": "b46965509996b764",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)",
   "id": "ae1043991b34ac81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = gemini.chat.completions.create(model=\"gemini-2.5-pro\", messages=[{\"role\":\"user\", \"content\": \"Tell me a funny story\"}])\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "5ebeaa195e74a239",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using Ollama locally ü¶ô\n",
    "\n",
    "In contrast to Gemini, Ollama allows us to run models **entirely on our local machine**.\n",
    "\n",
    "Before running any code that uses Ollama, make sure the local server is active.\n"
   ],
   "id": "7f53e76519fe36f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check local server\n",
    "requests.get(\"http://localhost:11434\").content"
   ],
   "id": "3953e85b76e6965c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If the output is not `Ollama is Running`, then:\n",
    "\n",
    "### Step 1 ‚Äì Start the Ollama server\n",
    "\n",
    "Open a terminal and run: `ollama serve`\n",
    "\n",
    "### Step 2 ‚Äì Ensure a model is installed\n",
    "\n",
    "Run: `ollama list`\n",
    "\n",
    "If needed, download a model, running: `ollama pull llama3.2`"
   ],
   "id": "4e2812f4515fefc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "\n",
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=[{\"role\":\"user\", \"content\":\"Tell me a fairy story!\"}])\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "f038453ba2542a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üß† Important to notice\n",
    "\n",
    "This exercise shows that LLM interaction patterns are largely independent\n",
    "from the underlying model family or deployment strategy.\n",
    "Although the code structure may look similar, the execution model is very different:\n",
    "\n",
    "- **Gemini** ‚Üí remote inference, API-based, managed infrastructure\n",
    "- **Ollama** ‚Üí local inference, no API keys, full control over data\n",
    "\n",
    "This distinction is fundamental when reasoning about:\n",
    "- privacy,\n",
    "- latency,\n",
    "- costs,\n",
    "- deployment constraints."
   ],
   "id": "50901f735dc19be9"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

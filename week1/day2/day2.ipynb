{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise Overview â€“ Local LLM Inference with Ollama ðŸ¦™\n",
    "\n",
    "The objective of this exercise is the same as in the previous one:\n",
    "to **extract textual content from a website and generate a summary using a Large Language Model**.\n",
    "\n",
    "The key difference is that, in this case, the language model is **not accessed through a cloud API**, but **runs locally** using **Ollama**.\n",
    "\n",
    "The workflow is therefore:\n",
    "1. fetch a web page given its URL,\n",
    "2. process and clean the HTML content using **BeautifulSoup**,\n",
    "3. send the extracted text to a **locally running LLM**,\n",
    "4. generate a summary directly on the local machine.\n",
    "\n",
    "---\n",
    "\n",
    "## Running the model locally\n",
    "\n",
    "This exercise requires **Ollama** to be installed on the system.\n",
    "\n",
    "Once installed, a model can be started from the terminal using:\n",
    "\n",
    "```bash\n",
    "ollama run model-name\n",
    "```\n",
    "\n",
    "In this notebook, the model used is:\n",
    "```bash\n",
    "ollama run llama3.2\n",
    "```\n",
    "\n",
    "Running the model locally highlights an important aspect of generative AI experimentation:\n",
    "> large language models can be used without external APIs, enabling offline usage!\n",
    "\n"
   ],
   "id": "3772f3c394755e58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from week1.scraper import Website"
   ],
   "id": "d05824d836101ba5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "url = \"https://www.fairy-circles.info\"\n",
    "website_contents = Website(url)"
   ],
   "id": "ee796eaae241031",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb013e7485db0d37",
   "metadata": {},
   "source": [
    "# Step 1: Prompts\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents \\\n",
    "and provides a short summary, ignoring text that might be related. \\\n",
    "Respond in markdown.\"\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website_contents.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 2: Messages\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ],
   "id": "65917163f32a6319",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Step 3: Call ollama\n",
    "openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "response = openai.chat.completions.create(model=\"llama3.2\", messages=messages_for(website_contents))"
   ],
   "id": "e56f069b9b722041",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# Step 4: print the result\n",
    "print(website_contents.title)\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

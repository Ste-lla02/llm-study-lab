{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Understanding response.choices[0].message.content\n",
    "\n",
    "This notebook shows the same thing in two different ways:\n",
    "1. HTTP call (using `requests`) to the **Chat Completions API** entrypoint.\n",
    "2. `openai` call to build the same request.\n",
    "\n",
    "The key idea is that the model output is **not returned as a plain string**. The API returns a **JSON object**, and the generated text lives inside:\n",
    "```text\n",
    "response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "```\n",
    "or in the object form:\n",
    "\n",
    "```text\n",
    "response.choices[0].message.content\n",
    "```"
   ],
   "id": "b5f1ba51f38e80aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸŽ¯ Objective\n",
    "\n",
    "The goal is to **understand what a language model API really returns** when we send a prompt. so we need to focus on reading the API response *consciously*, not blindly.\n",
    "\n",
    "## ðŸ§© Prerequisites\n",
    "Read the `fun_fact` of the day! ðŸ˜Ž\n",
    "\n",
    "## ðŸ¤” Why this matters\n",
    "\n",
    "When working with LLM APIs, **the model does not return a simple string**.\n",
    "\n",
    "Instead, the API returns a **JSON object** that contains:\n",
    "- the generated text,\n",
    "- alternative candidate responses,\n",
    "- metadata about the request and the generation process.\n",
    "\n",
    "Understanding this structure is crucial because:\n",
    "- real applications must **parse and inspect responses**, not just print them;\n",
    "- advanced workflows (logging, filtering, ranking, evaluation) all depend on it."
   ],
   "id": "d54121337a608fea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ],
   "id": "84650e6b1d83d0bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "# Define the HTTP headers required by the API.\n",
    "# - Authorization: identifies the client using a Bearer token (API key)\n",
    "# - Content-Type: specifies that the request body is formatted as JSON\n",
    "headers = {\"Authorization\": f\"Bearer {api_key}\",\n",
    "           \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Build the request payload as a Python dictionary.\n",
    "payload = {\n",
    "    \"model\": \"gpt-5-nano\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a fun fact\"}]\n",
    "}\n",
    "\n",
    "payload"
   ],
   "id": "87427fddf3c9db18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Send the HTTP POST request to the Chat Completions API entrypoint.\n",
    "response = requests.post(\n",
    "    \"https://api.openai.com/v1/chat/completions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "# The API response is returned as an HTTP response object.\n",
    "response.json()"
   ],
   "id": "769db0473b2b7e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract the generated text from the API response.\n",
    "response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ],
   "id": "d100b2154ed9fdcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "# Create a client instance.\n",
    "openai = OpenAI()\n",
    "\n",
    "# Send a chat completion request to the API.\n",
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=[{\"role\":\"user\", \"content\": \"Hi GPT, tell me a strange story!\"}])\n",
    "\n",
    "# The API returns a structured object.\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "5bf1a03ba461a316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## âœ… Conclusion\n",
    "\n",
    "In this notebook, we moved one step closer to using Large Language Models **consciously** rather than mechanically.\n",
    "\n",
    "Instead of treating the model output as plain text, we explored the **structure of the API response** and learned where the generated content actually lives. This shift in perspective is essential: when working with LLMs in real applications, understanding the response format matters as much as writing a good prompt.\n",
    "\n",
    "The expression `response.choices[0].message.content` is not an arbitrary access pattern, but the final step of a well-defined data structure designed to support multiple outputs, metadata, and extensibility.\n",
    "\n",
    "From this point on, every interaction with an LLM API should be seen as:\n",
    "- a request to a service,\n",
    "- a structured response,\n",
    "- and a deliberate extraction of the information we need."
   ],
   "id": "1516f848e18b7792"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ§  The Illusion of Memory in Chat-Based LLMs\n",
    "\n",
    "When interacting with ChatGPT or similar systems, it feels like the model remembers what we said earlier.\n",
    "\n",
    "We say our name once, ask a follow-up question later, and the model answers correctly. Creating so a strong illusion of memory. In reality, Large Language Models have no memory at all.\n",
    "\n",
    "This notebook demonstrates why.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” A key idea to keep in mind\n",
    "> Every API call to an LLM is completely stateless.\n",
    "\n",
    "The model: does not remember previous calls, does not store user information, does not know what happened â€œbeforeâ€ unless we explicitly tell it.\n",
    "Each request is handled independently.\n",
    "\n",
    "---"
   ],
   "id": "dbc239db8adc9cce"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-10T17:06:37.039200Z",
     "start_time": "2026-02-10T17:06:37.034139Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§ª First experiment: a single-turn interaction\n",
    "Letâ€™s start with a simple message exchange where the model responds politely and may even greet us by name."
   ],
   "id": "13412e8ee9fe2b7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:06:40.970974Z",
     "start_time": "2026-02-10T17:06:38.465494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello There! I'm Stella\"}\n",
    "    ]\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ],
   "id": "b0ff01cbd91f67bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Stella! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## â“ Second experiment: a follow-up question\n",
    "Now we ask a second question in a new API call, but we do not include the previous message.\n",
    "Here the models replies with something like:\n",
    "```text\n",
    "Iâ€™m sorry, but I donâ€™t know your name.\n",
    "```"
   ],
   "id": "75ee1a4f4bccfcce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:06:44.851172Z",
     "start_time": "2026-02-10T17:06:43.339940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ],
   "id": "928e16d053149198",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iâ€™m sorry, but I donâ€™t know your name. Could you please tell me?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ¤¯ What just happened?\n",
    "\n",
    "Nothing went wrong. Indeed, this behavior is **correct**.\n",
    "> The model did not forget your name â€” it never knew it in the first place.\n",
    "\n",
    "Why? Because:\n",
    "- the second API call did not include the earlier message\n",
    "- the model only sees the input it receives **right now**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© How chat systems create the illusion of memory\n",
    "Chat-based applications (like ChatGPT) use a simple but powerful trick:\n",
    "> They resend the entire conversation history at every turn.\n",
    ">\n",
    "Instead of sending only the latest user message, they send: the system prompt, all previous user messages, all previous assistant responses, and the new user message.\n",
    "All of this becomes one long input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Recreating â€œmemoryâ€ manually\n",
    "Letâ€™s try again, this time including the full conversation."
   ],
   "id": "9335deae5682bf16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T17:06:48.539081Z",
     "start_time": "2026-02-10T17:06:47.228026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Stella!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Stella! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "response.choices[0].message.content"
   ],
   "id": "4c326e2a0f28da91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Stella! How can I help you today?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ’¸ Why context affects cost\n",
    "Because the entire conversation is resent every time:\n",
    "- longer conversations â†’ more input tokens\n",
    "- more input tokens â†’ more computation\n",
    "- more computation â†’ higher cost\n",
    "-\n",
    "This is not a bug â€” itâ€™s the price of maintaining context.\n",
    "You are literally asking the model to reprocess the entire past conversation at every turn.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Connection with tokenization\n",
    "This notebook connects directly to what we learned earlier:\n",
    "- conversation history = more tokens\n",
    "- more tokens = higher cost\n",
    "t- oken limits define how much â€œmemoryâ€ you can simulate.\n",
    "\n",
    "In practice:\n",
    "> LLM memory is just tokenized text that you keep sending back."
   ],
   "id": "79abf71b9800be5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLMs Q&A Challenge ðŸ’¬\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "The goal is to compare the behavior of **multiple Large Language Models (LLMs)** under the same prompts, reusing code and concepts introduced in Week 1.\n",
    "\n",
    "In particular, four models are compared: gpt-5-nano, llama3.2, gemini-2.5-pro, and deepseek-r1:1.5b\n",
    "\n",
    "The â€œchallengeâ€ consists of asking each model **the same set of questions**, including:\n",
    "- deliberately **nonsensical or ambiguous** prompts,\n",
    "- **metalinguistic** requests (e.g., word counting),\n",
    "- **abstract descriptive** questions,\n",
    "- **self-reflective** questions about the modelâ€™s own capabilities and limitations.\n",
    "\n",
    "The objective is not to identify a â€œwinner,â€ but rather to observe **qualitative differences** in how models interpret, handle, and respond to the same input.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Requirements\n",
    "\n",
    "To run the notebook correctly, the following are required:\n",
    "\n",
    "- The following API keys (when required by the selected models):\n",
    "  - OpenAI API key\n",
    "  - Google Gemini API key\n",
    "- Ollama installed and running locally for locally executed models\n",
    "  (e.g., `ollama serve` from the terminal)\n",
    "\n",
    "API keys must be loaded via a `.env` file, as shown in the previous notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Observations\n",
    "\n",
    "When analyzing the responses, it is important to keep in mind several key aspects:\n",
    "\n",
    "- **Same prompt â‰  same context**\n",
    "  Even with identical textual input, models operate under different conditions:\n",
    "  tokenizers, implicit system prompts, safety policies, and generation strategies are not uniform. These factors directly affect the structure and content of the responses.\n",
    "\n",
    "- **Nonsense questions and handling of absurdity**\n",
    "  Intentionally meaningless prompts make it possible to observe whether a model:\n",
    "  - attempts to â€œinventâ€ an answer anyway (hallucination),\n",
    "  - explicitly signals that the question has no meaning,\n",
    "  - freely reinterprets or reformulates the prompt.\n",
    "\n",
    "- **Instability of metalinguistic requests**\n",
    "  Questions such as *â€œHow many words are there in your answer to this question?â€* are inherently unstable:\n",
    "  word counts may vary depending on punctuation, spacing, reformulations, and even on different versions of the generated response.\n",
    "\n",
    "These observations help illustrate that LLM behavior does not depend solely on model â€œpower,â€ but also on **architectural and design choices** that are often invisible to the end user.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ External reference\n",
    "\n",
    "For those interested in seeing language models tested in a more playful setting, there is also an interesting and entertaining project available at:\n",
    "\n",
    "ðŸ‘‰ https://edwarddonner.com/outsmart/\n",
    "\n",
    "It is a game of **strategy and diplomacy** in which different LLMs compete directly, revealing how strategies, alliances, and communication failures emerge depending on the model being used. This provides a compelling example of LLM comparison beyond a purely technical context.\n"
   ],
   "id": "ea4fd3427cfa1a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import time"
   ],
   "id": "1ab9fb288ee0bbbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# Gemini: gemini-2.5-pro\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert google_api_key, \"Missing GOOGLE_API_KEY in .env\"\n",
    "\n",
    "#OpenAI: gpt-5-nano\n",
    "OPENAI_BASE_URL = \"https://api.openai.com/v1/\"\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "assert open_api_key, \"Missing OPENAI_API_KEY in .env\"\n",
    "\n",
    "#Ollama: llama3.2 and deepseek-r1:1.5b\n",
    "OLLAMA_BASE_URL=\"http://localhost:11434/v1\"\n",
    "ollama_api_key=\"ollama\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "QUESTIONS = [\n",
    "    \"how many rainbows does it take to jump from Hawaii to 17?\",\n",
    "    \"how many words are there in your answer to this question?\",\n",
    "    \"how would you describe the color red to someone who's never been able to see?\",\n",
    "    \"Compared with other Frontier LLMs, what kinds of questions are you best at answering, \"\n",
    "     \"and what kinds of questions do you find most challenging? Which other LLM has capabilities that complement yours?\"\n",
    "]"
   ],
   "id": "4834070de7234b54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODELS = [\n",
    "    {\"model_name\":\"gemini-2.5-pro\",\n",
    "     \"base_url\":GEMINI_BASE_URL,\n",
    "     \"api_key\":google_api_key},\n",
    "    {\"model_name\":\"llama3.2\",\n",
    "     \"base_url\":OLLAMA_BASE_URL,\n",
    "     \"api_key\":ollama_api_key},\n",
    "    {\"model_name\":\"deepseek-r1:1.5b\",\n",
    "     \"base_url\":OLLAMA_BASE_URL,\n",
    "     \"api_key\":ollama_api_key},\n",
    "    {\"model_name\":\"gpt-5-nano\",\n",
    "     \"base_url\": OPENAI_BASE_URL,\n",
    "     \"api_key\": open_api_key}]"
   ],
   "id": "5a3d9110bc4a4fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's try!\n",
    "for model in MODELS:\n",
    "    print(model[\"model_name\"])\n",
    "    client = OpenAI(base_url=model[\"base_url\"], api_key=model[\"api_key\"])\n",
    "    response = client.chat.completions.create(model=model[\"model_name\"], messages=[{\"role\":\"user\", \"content\":\"hi\"}])\n",
    "    print(response.choices[0].message.content)"
   ],
   "id": "2885452962829c53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to end a single question to a language model and return its response.\n",
    "def response_instance(model, question):\n",
    "    client = OpenAI(base_url=model[\"base_url\"], api_key=model[\"api_key\"])\n",
    "    t0 = time.time()\n",
    "    response = client.chat.completions.create(model=model[\"model_name\"], messages=[{\"role\":\"user\", \"content\":question}])\n",
    "    latency = time.time() - t0\n",
    "    return response, latency\n",
    "\n",
    "# Function to count the number of words in a text string.\n",
    "def word_count(s: str) -> int:\n",
    "    return len([w for w in s.strip().split() if w])"
   ],
   "id": "649d5b22ec5ed3d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to run the multi-model question-answering challenge.\n",
    "def run_challenge(models, questions) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for q, question in enumerate(questions, start=1):\n",
    "        for model in models:\n",
    "            try:\n",
    "                response, latency = response_instance(model, question)\n",
    "                text = response.choices[0].message.content\n",
    "                rows.append({\n",
    "                    \"q_id\": q,\n",
    "                    \"question\": question,\n",
    "                    \"model\": model[\"model_name\"],\n",
    "                    \"latency_s\": latency,\n",
    "                    \"word_count\": word_count(text),\n",
    "                    \"answer\": text,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                rows.append({\n",
    "                    \"q_id\": q,\n",
    "                    \"question\": question,\n",
    "                    \"model\": model[\"model_name\"],\n",
    "                    \"latency_s\": None,\n",
    "                    \"word_count\": None,\n",
    "                    \"answer\": f\"[ERROR] {type(e).__name__}: {e}\",\n",
    "                })\n",
    "    return pd.DataFrame(rows)"
   ],
   "id": "dae2ac3b6d0046f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the challange\n",
    "df = run_challenge(MODELS, QUESTIONS)"
   ],
   "id": "ff2089ae3e2b7519",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a compact summary table with basic quantitative metadata\n",
    "summary = df[[\"q_id\", \"model\", \"latency_s\", \"word_count\"]].sort_values([\"q_id\", \"model\"])\n",
    "summary"
   ],
   "id": "ea6ed5a5e695086a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to display model responses for a single question in a readable, ordered format.\n",
    "def show_answers(df: pd.DataFrame, q_id: int):\n",
    "    block = df[df[\"q_id\"] == q_id].sort_values(\"model\")\n",
    "    print(f\"\\nQ{q_id}: {block.iloc[0]['question']}\\n\" + \"-\"*80)\n",
    "    for _, r in block.iterrows():\n",
    "        print(f\"\\n### {r['model']} | {r['latency_s']:.2f}s | {r['word_count']} words\\n\")\n",
    "        print(r[\"answer\"])\n",
    "\n",
    "show_answers(df, 1)"
   ],
   "id": "32ddb3f69f323359",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final note\n",
    "\n",
    "This experiment highlights how Large Language Models differ not only in\n",
    "knowledge and reasoning style, but also in how they interpret ill-defined,\n",
    "self-referential, or nonsensical prompts. Even simple metrics such as word\n",
    "count or response length can vary significantly, reminding us that LLM\n",
    "behavior is deeply shaped by architectural choices, training data, and\n",
    "inference-time policies."
   ],
   "id": "28b65923e4d184f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
